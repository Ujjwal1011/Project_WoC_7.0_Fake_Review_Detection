{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Model Traning And Pipeline Creation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Building pipeline With TfIDF Vectorization***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Importing Library***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01memoji\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_config\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'emoji'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import joblib\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure NLTK downloads are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load DataSet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"C:\\\\Users\\\\ujjwa_n18433z\\\\Desktop\\\\ujjwal\\\\All Projects\\\\Project_WoC_7.0_Fake_Review_Detection\\\\fakeReviewData.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Text Preprocessor Class**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(self._preprocess_text)\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        # Convert emojis to text\n",
    "        text = emoji.demojize(text, delimiters=(\"\", \"\"))\n",
    "\n",
    "        # Handle contractions\n",
    "        text = re.sub(r\"can't\", \"can not\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"n't\", \" not\", text)\n",
    "        text = re.sub(r\"'m\", \" am\", text)\n",
    "        text = re.sub(r\"'s\", \" is\", text)\n",
    "        text = re.sub(r\"'re\", \" are\", text)\n",
    "        text = re.sub(r\"'ve\", \" have\", text)\n",
    "        text = re.sub(r\"'d\", \" would\", text)\n",
    "        text = re.sub(r\"'ll\", \" will\", text)\n",
    "\n",
    "        # Remove URLs and email addresses\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "        text = re.sub(r\"\\S*@\\S*\\s?\", \"\", text)\n",
    "\n",
    "        # Remove special characters\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "        # Remove digits and lowercase\n",
    "        text = \"\".join([char.lower() for char in text if not char.isdigit()])\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Tokenize\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in self.stopwords]\n",
    "\n",
    "        # Lemmatize\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
    "\n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r\"\\s+\", \" \", \" \".join(words)).strip()\n",
    "\n",
    "        # Rejoin words into a single string\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Encoding Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = LabelEncoder()\n",
    "ds['label'] = target_encoder.fit_transform(ds['label'])  # Encode target (e.g., 'CG' -> 0, 'OR' -> 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mapping\n",
    "dict(zip(target_encoder.classes_, range(len(target_encoder.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating a Column Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', Pipeline([\n",
    "            ('preprocessor', TextPreprocessor()),\n",
    "            ('tfidf', TfidfVectorizer(max_features=500))\n",
    "        ]), 'text_'),\n",
    "        ('category', OneHotEncoder(handle_unknown='ignore'), ['category']),\n",
    "        ('rating', StandardScaler(), ['rating'])  # Wrap 'rating' in a list to make it 2D\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Split the data into training and testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = ds.drop(columns=['label']) \n",
    "y = ds['label'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define Full Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinelg = Pipeline([\n",
    "    ('preprocessor', preprocessor),                    \n",
    "    ('classifier', LogisticRegression(random_state=42))  # Logistic Regression\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinelg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train, predict and evaluate Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline\n",
    "pipelinelg.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = pipelinelg.predict(X_test)\n",
    "\n",
    "# Create Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipelinelg.named_steps['classifier'].classes_)\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define Support Vector Classifier Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipelinesvm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train, predict and evaluate SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline\n",
    "pipelinesvm.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = pipelinesvm.predict(X_test)\n",
    "\n",
    "# Create Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipelinesvm.named_steps['classifier'].classes_)\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define Random Forest Classifier Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipelinerf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinerf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train, predict and evaluate Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinerf.fit(X_train, y_train)\n",
    " \n",
    "# Generate predictions\n",
    "y_pred = pipelinerf.predict(X_test)\n",
    "\n",
    "# Create Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipelinerf.named_steps['classifier'].classes_)\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define Multi Layer Perceptron Classifier Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipelinemlp = Pipeline([\n",
    "    ('preprocessor', preprocessor),  \n",
    "    ('classifier', MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        activation='relu',           \n",
    "        solver='adam',               \n",
    "        max_iter=200,                \n",
    "        random_state=42,\n",
    "        early_stopping=True,         \n",
    "        validation_fraction=0.1      \n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinemlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train, predict and evaluate MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline\n",
    "pipelinemlp.fit(X_train, y_train)\n",
    " \n",
    "# Generate predictions\n",
    "y_pred = pipelinemlp.predict(X_test)\n",
    "\n",
    "# Create Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipelinemlp.named_steps['classifier'].classes_)\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Building Pipeline With Embedding*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Word2Vec Model Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(self._preprocess_text)\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        # Convert emojis to text\n",
    "        text = emoji.demojize(text, delimiters=(\"\", \"\"))\n",
    "\n",
    "        # Handle contractions\n",
    "        text = re.sub(r\"can't\", \"can not\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"n't\", \" not\", text)\n",
    "        text = re.sub(r\"'m\", \" am\", text)\n",
    "        text = re.sub(r\"'s\", \" is\", text)\n",
    "        text = re.sub(r\"'re\", \" are\", text)\n",
    "        text = re.sub(r\"'ve\", \" have\", text)\n",
    "        text = re.sub(r\"'d\", \" would\", text)\n",
    "        text = re.sub(r\"'ll\", \" will\", text)\n",
    "\n",
    "        # Remove URLs and email addresses\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "        text = re.sub(r\"\\S*@\\S*\\s?\", \"\", text)\n",
    "\n",
    "        # Remove special characters\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "        # Remove digits and lowercase\n",
    "        text = \"\".join([char.lower() for char in text if not char.isdigit()])\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Tokenize\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "        # Lemmatize\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
    "\n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r\"\\s+\", \" \", \" \".join(words)).strip()\n",
    "\n",
    "        # Rejoin words into a single string\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model=None, vector_size=100):\n",
    "        self.model = model\n",
    "        self.vector_size = vector_size\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.model is None:\n",
    "            sentences = [text.split() for text in X]\n",
    "            self.model = Word2Vec(sentences, vector_size=self.vector_size, window=5, min_count=1, workers=4)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        vectors = []\n",
    "        for text in X:\n",
    "            words = text.split()\n",
    "            word_vectors = [self.model.wv[word] for word in words if word in self.model.wv]\n",
    "            if word_vectors:\n",
    "                vectors.append(np.mean(word_vectors, axis=0))\n",
    "            else:\n",
    "                vectors.append(np.zeros(self.vector_size))  # Return a zero vector if no words are found\n",
    "        return np.array(vectors, dtype=np.float32)  # Ensure output is of type float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define Column Transformer with Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', Pipeline([\n",
    "            ('preprocessor', TextPreprocessor()),\n",
    "            ('word2vec', Word2VecTransformer(vector_size=300))\n",
    "        ]), 'text_'),\n",
    "        ('category', OneHotEncoder(handle_unknown='ignore'), ['category']),\n",
    "        ('rating', StandardScaler(), ['rating'])  \n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define MLP Pipeline with Word2Vec Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipelinemlp_word2vec = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Your ColumnTransformer\n",
    "    ('classifier', MLPClassifier(\n",
    "        hidden_layer_sizes=(100,50),  # Example: 2 hidden layers, 100 and 50 neurons\n",
    "        activation='relu',              # ReLU activation\n",
    "        solver='adam',                 # Adam optimizer\n",
    "        max_iter=200,                  # Increase if needed for convergence\n",
    "        random_state=42,\n",
    "        early_stopping=True,          # Use early stopping to prevent overfitting\n",
    "        validation_fraction=0.1       # Proportion of training data for validation\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinemlp_word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train, predict and evaluate MLP with Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline\n",
    "pipelinemlp_word2vec.fit(X_train, y_train)\n",
    " \n",
    "# Generate predictions\n",
    "y_pred = pipelinemlp_word2vec.predict(X_test)\n",
    "\n",
    "# Create Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.array(['CG', 'OR']))\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Saving Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipelinemlp, 'best_mlp_word2vec_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Building Pipeline With Spelling Correction and Embedding*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Spelling Correction Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Spelling Correction Transformer ---\n",
    "class SpellingCorrectionTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lang='en'): # add language\n",
    "        self.spell = Speller(lang=lang) # initialize Speller\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Nothing to fit, so just return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        corrected_texts = []\n",
    "        for text in X:\n",
    "            # 1. Correct Spelling\n",
    "            corrected_words = [self.spell(word) for word in text.split()] \n",
    "            corrected_text = \" \".join(corrected_words)  \n",
    "\n",
    "            corrected_texts.append(corrected_text)  \n",
    "        return corrected_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define Column Transformer with Spelling Correction ,Text Processing ,Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ColumnTransformer ---\n",
    "text_column = 'text_'\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', Pipeline([\n",
    "            ('spelling', SpellingCorrectionTransformer()),\n",
    "            ('preprocessor', TextPreprocessor()),\n",
    "            ('word2vec', Word2VecTransformer(vector_size=150))\n",
    "        ]), text_column),\n",
    "        ('category', OneHotEncoder(handle_unknown='ignore',drop=\"first\"), ['category']),\n",
    "        ('rating', StandardScaler(), ['rating'])\n",
    "    ],\n",
    "    remainder='passthrough'  # or 'drop' if you don't want to keep remaining columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define MLP Pipeline with Spelling Correction ,Text Processing ,Word2Vec Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinemlp_sc_word2vec = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model',MLPClassifier(\n",
    "    hidden_layer_sizes=(150, 70,30),  \n",
    "    activation='relu',              \n",
    "    solver='adam',                \n",
    "    max_iter=200,                  \n",
    "    random_state=42,\n",
    "    early_stopping=True,          \n",
    "    validation_fraction=0.1       \n",
    "))  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train, predict and evaluate MLP with Spelling Correctin and Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Train and Evaluate MLPClassifier ---\n",
    "pipelinemlp_sc_word2vec.fit(X_train, y_train)\n",
    "y_pred = pipelinemlp_sc_word2vec.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.array(['CG', 'OR']))\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"MLP Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"MLP Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Building Pipeline With Pos Tagging and Embedding***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- POS Tagger Transformer ---\n",
    "class PosTaggerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        pos_tags = []\n",
    "        for text in X:\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            tags = nltk.pos_tag(tokens)  \n",
    "            pos_tags.append([tag for word, tag in tags])\n",
    "        return pos_tags\n",
    "    \n",
    "\n",
    "# --- POS Encoder Transformer ---\n",
    "class PosEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False,drop='first')\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        all_pos_tags = [tag for sublist in X for tag in sublist] \n",
    "        self.ohe.fit(np.array(all_pos_tags).reshape(-1, 1)) \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"PosEncoderTransformer must be fitted before transform.\")\n",
    "        encoded_pos = []\n",
    "        for doc_pos_tags in X:\n",
    "            encoded_tags_for_doc = self.ohe.transform(np.array(doc_pos_tags).reshape(-1, 1))\n",
    "            encoded_pos.append(encoded_tags_for_doc.mean(axis=0)) #average the encodings\n",
    "        return np.array(encoded_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, new_column_name, source_column):  \n",
    "        self.new_column_name = new_column_name\n",
    "        self.value = source_column\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        # Apply a constant value\n",
    "        X[self.new_column_name] = self.value\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ColumnTransformer ---\n",
    "text_column = 'text_'\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('word2vec', Pipeline([\n",
    "            ('preprocessor', TextPreprocessor()),\n",
    "            ('word2vec', Word2VecTransformer(vector_size=150)) \n",
    "        ]), text_column),\n",
    "\n",
    "        ('pos', Pipeline([\n",
    "            ('preprocessor', TextPreprocessor()),  \n",
    "            ('pos_tagger', PosTaggerTransformer()), \n",
    "            ('pos_encoder', PosEncoderTransformer())\n",
    "        ]), 'text_pos'),\n",
    "\n",
    "        ('category', OneHotEncoder(handle_unknown='ignore', drop=\"first\"), ['category']),\n",
    "        ('rating', StandardScaler(), ['rating'])\n",
    "    ],\n",
    "    remainder='passthrough'  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_classifier = Pipeline([\n",
    "    ('create_text_pos', ColumnCreator(new_column_name='text_pos',source_column='text_')), \n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', MLPClassifier(\n",
    "        hidden_layer_sizes=(150, 70, 30),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=200,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train and Evaluate MLPClassifier ---\n",
    "\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "y_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"MLP Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"MLP Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
